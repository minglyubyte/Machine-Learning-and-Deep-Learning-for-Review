{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e440d75d-e0b0-4d44-b988-ea4e996d7e5d",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5c339-9c48-4a56-8dde-5ef024a49de4",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d969e-477d-431a-8962-04774c9b74dd",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used in machine learning and statistics to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d085b78-3f6d-4c1b-b5c7-aea3d04331ee",
   "metadata": {},
   "source": [
    "$$Y = β₀ + β₁X + ε$$\n",
    "\n",
    "Where:\n",
    "- **Y** is the dependent variable (the one you want to predict).\n",
    "- **X** is the independent variable (the one used for prediction).\n",
    "- **β₀** is the intercept (the value of Y when X is 0).\n",
    "- **β₁** is the slope (the change in Y for a one-unit change in X).\n",
    "- **ε** represents the error term\n",
    "\n",
    "Linear regression can be extended to multiple independent variables, creating multiple linear regression, where the equation becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4b27b-e7a8-4158-a913-51e39fa9ae94",
   "metadata": {},
   "source": [
    "$$Y = a + b_1X_1 + b_2X_2 + ... + b_nX_n$$\n",
    "In this case, there are **n** independent variables, and each has its own coefficient **b** that represents its contribution to the prediction of the dependent variable **Y**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384b5fb-ed2e-4739-97db-513cd3a1f8ca",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7e07d-99b3-4986-9635-80b943fdf5bd",
   "metadata": {},
   "source": [
    "Linear regression relies on several fundamental assumptions to be valid and produce reliable results. These assumptions are crucial for the proper interpretation of regression analyses:\n",
    "\n",
    "1. **Linearity**: The relationship between the dependent variable (**Y**) and the independent variables (**X<sub>1</sub>**, **X<sub>2</sub>**, ..., **X<sub>n</sub>**) should be linear. This means that changes in the independent variables should result in proportional changes in the dependent variable. You can assess linearity through scatterplots and residual plots.\n",
    "\n",
    "2. **Independence of Errors**: The errors (residuals) from the regression model should be independent of each other. In other words, the value of the residual for one observation should not depend on the values of residuals for other observations. Autocorrelation or time-series data can violate this assumption.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals should be constant across all levels of the independent variables. This implies that the spread of residuals should not change as you move along the predictor values. Heteroscedasticity, where the spread of residuals varies, can lead to biased standard errors and affect the validity of hypothesis tests.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals should follow a normal distribution. While this assumption is not necessary for large sample sizes due to the Central Limit Theorem, it is important for small sample sizes. Deviations from normality can affect the accuracy of confidence intervals and p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322d6d-9b90-43bc-9cc0-d812afed1804",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef168fba-3907-48cb-9e59-1b159e1ade0b",
   "metadata": {},
   "source": [
    "Certainly! Here's the explanation of Ordinary Least Squares (OLS) in a simplified Markdown format:\n",
    "\n",
    "### Ordinary Least Squares (OLS) in Linear Regression\n",
    "\n",
    "In linear regression, Ordinary Least Squares (OLS) is a widely-used method to estimate the parameters of a linear model. The primary goal of OLS is to find the best-fitting line through a set of data points by minimizing the sum of squared differences between observed values and predicted values.\n",
    "\n",
    "OLS aims to find the values of **β₀** and **β₁** that minimize the sum of squared residuals (SSR), given by:\n",
    "\n",
    "$$SSR = \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 X_i))^2$$\n",
    "\n",
    "In this equation:\n",
    "- **Y<sub>i</sub>** is the observed value of the dependent variable for the i-th data point.\n",
    "- **X<sub>i</sub>** is the corresponding value of the independent variable.\n",
    "- **n** is the number of data points.\n",
    "\n",
    "We seek to find solution to minimize the SSR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79319429-4d1a-4d73-b387-d602aab6fd30",
   "metadata": {},
   "source": [
    "$$Y = X\\beta + \\varepsilon $$\n",
    "\n",
    "\n",
    "The goal is to estimate the coefficients **β** that minimize the sum of squared residuals:\n",
    "\n",
    "$$SSR = \\varepsilon^T \\varepsilon = (Y - X\\beta)^T (Y - X\\beta)$$\n",
    "\n",
    "To find the OLS solution, we take the derivative of SSR with respect to \\( \\beta \\) and set it equal to zero:\n",
    "\n",
    "$$ \\frac{\\partial SSR}{\\partial \\beta} = -2X^T(Y - X\\beta) = 0$$\n",
    "\n",
    "Solving for **β** gives the OLS estimator:\n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^TY$$\n",
    "\n",
    "We had the answer for optimal solution of Linear regression using OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c5627-885f-428e-a3ac-6593d56af4cc",
   "metadata": {},
   "source": [
    "## Pros, Cons and Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc97d3-c1fd-48e6-9d19-eddbdeca4938",
   "metadata": {},
   "source": [
    "### Pros of Linear Regression\n",
    "\n",
    "1. **Simplicity**: Linear regression is straightforward to understand and explain, making it a good starting point for predictive modeling.\n",
    "2. **Efficient Computation**: It requires relatively less computational resources compared to more complex algorithms.\n",
    "3. **Interpretable Results**: The output of a linear regression model can be easily interpreted in terms of relationship strength and direction between variables.\n",
    "4. **Basis for Other Methods**: Serves as a foundation for understanding more complex models in machine learning.\n",
    "5. **Less Prone to Overfitting**: With fewer variables, linear regression models are less likely to fit noise in the data.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Cons of Linear Regression\n",
    "\n",
    "1. **Assumption of Linearity**: Linear regression assumes a linear relationship between the dependent and independent variables, which is not always the case in real-world data.\n",
    "2. **Sensitive to Outliers**: Outliers can significantly affect the regression line and hence the forecasted values.\n",
    "3. **Multicollinearity**: The presence of high correlation between independent variables can distort the estimated coefficients and make them unreliable.\n",
    "4. **Limited to Continuous Variables**: Linear regression is typically used for continuous numerical data, limiting its use with categorical data.\n",
    "5. **Can’t Model Complex Relationships**: It cannot capture non-linear relationships without transformation of variables.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### When to use linear regression\n",
    "\n",
    "**Emphasis on Inference**\n",
    "- **Primary Goal**: Inference is the main objective. Linear regression is often superior for inferential purposes compared to other machine learning models.\n",
    "- **Insights and Estimates**: Provides detailed estimates on how features influence the outcome variable, complete with confidence intervals and statistical tests for a thorough understanding.\n",
    "\n",
    "**Ideal as a Baseline Model**\n",
    "- **Simplicity and Comparison**: Serves as an uncomplicated baseline for comparing more complex models.\n",
    "- **Advantages in Clean Data**: Particularly effective with datasets having minimal missing values or outliers.\n",
    "- **No Hyperparameter Tuning**: A significant advantage is the absence of hyperparameter tuning, simplifying the model development process.\n",
    "\n",
    "**Building Stakeholder Trust**\n",
    "- **Familiarity and Credibility**: Linear regression's well-established nature makes it a trustworthy choice among stakeholders initially skeptical of complex machine learning models.\n",
    "- **Step towards Advanced Modeling**: Once the linear regression model is accepted, it sets the stage for introducing and comparing more advanced models, demonstrating additional business value.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### When not to use linear regression\n",
    "\n",
    "**Impact of Small Predictive Improvements**\n",
    "- **Business Impact**: In scenarios where minor improvements in predictive accuracy can significantly affect business outcomes, exploring models beyond linear regression is advisable.\n",
    "- **Alternative Models for Better Performance**: Models like gradient boosted trees often outperform linear regression, especially when relationships between features and outcome variables aren't perfectly linear.\n",
    "\n",
    "**Time Constraints in Data Exploration**\n",
    "- **Challenges with Linear Regression**: Linear regression can be adversely affected by issues like missing data, outliers, and correlated features.\n",
    "- **Suitable Alternatives**: In situations with limited time for data cleaning and preprocessing, tree-based models such as random forests are preferable due to their resilience to these data issues.\n",
    "\n",
    "**Situations with More Features Than Observations**\n",
    "- **Inappropriateness of Standard Linear Regression**: When the number of features exceeds the number of observations, standard linear regression is not suitable.\n",
    "- **Solutions**: Opt for feature reduction strategies or models capable of handling high feature-to-observation ratios, such as ridge regression.\n",
    "\n",
    "**Handling Many Correlated Features**\n",
    "- **Limitation in Standard Regression**: Standard regression models struggle with multiple correlated features.\n",
    "- **Better Option**: Ridge regression, a regularized version of linear regression, effectively manages correlated features and offers a more robust solution in such cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f64bc1-7a41-45e0-a188-bf5c4bc57710",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95671375-17a4-45ef-ab99-c742a7d6bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43938e20-319f-45ff-bb99-74141166624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|███████████████████████| 245/245 [00:00<00:00, 404kB/s]\n",
      "/Users/leo/.pyenv/versions/3.10.13/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Downloading data: 100%|█████████████████████| 92.0k/92.0k [00:00<00:00, 681kB/s]\n",
      "Generating train split: 1599 examples [00:00, 86367.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lvwerra/red-wine\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79728ca6-bf9b-481b-87a9-659bf73c5313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "    num_rows: 1599\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054902c6-bad7-4928-a63a-4f5610a54d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
