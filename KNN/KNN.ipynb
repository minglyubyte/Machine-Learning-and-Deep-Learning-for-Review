{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2075c8c6-936d-4630-acbb-410d389bd005",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c01c7a-eec6-40a5-bfec-f91254917c4d",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8ea77-cb38-4bb9-9e01-45ae9df8f413",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple, versatile, and widely used algorithm in machine learning for both classification and regression. It's a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation.\n",
    "\n",
    "### Working Principle of KNN:\n",
    "\n",
    "In both classification and regression, KNN works on the principle of feature similarity: How closely out-of-sample features resemble our training set determines how we classify a given data point or predict a continuous value.\n",
    "\n",
    "1. **Classification**: An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its $ k $ nearest neighbors (where $ k $ is a positive integer, typically small). If $ k = 1 $, the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "2. **Regression**: In the case of regression, KNN predicts the output based on the average of the values of its $ k $ nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0580063-fc14-4059-8b4a-f10f32d8ed43",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e19f6-607f-4196-86ca-27f4ac0dd7e1",
   "metadata": {},
   "source": [
    "1. **Homogeneity in Feature Space**: KNN assumes that similar data points exist in close proximity to each other in the feature space. In other words, the algorithm relies on the assumption that closer data points in the feature space are more similar to each other than those farther away.\n",
    "\n",
    "2. **Feature Scaling**: KNN is sensitive to the scale of the data, with an assumption that all features contribute equally to the similarity. Therefore, features need to be normalized or standardized for the algorithm to work correctly.\n",
    "\n",
    "3. **Labelled Data**: KNN is a supervised learning algorithm, assuming the presence of labelled training data.\n",
    "\n",
    "4. **Curse of Dimensionality**: KNN assumes that the data isn't too high-dimensional, as the algorithm doesn't perform well with a high number of features due to the \"curse of dimensionality.\" The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings. K-Nearest Neighbors (KNN) is particularly affected by this curse for several reasons:\n",
    "\n",
    "    1. **Distance Metric Becomes Less Informative**: In high-dimensional spaces, the concept of \"nearness\" or \"distance\" can become less meaningful. The Euclidean distance (or other metrics) tends to become more uniform across different pairs of points as the number of dimensions increases. This uniformity makes it difficult for KNN to distinguish between near and far neighbors because all points start to appear almost equally far from each other.\n",
    "\n",
    "    2. **Sparsity of Data**: As the dimensionality increases, the volume of the space increases so rapidly that the available data become sparse. This sparsity means that any given observation has very few close neighbors. KNN relies on finding nearby points, but in high dimensions, all points are likely far away from each other, leading to less reliable predictions.\n",
    "\n",
    "    3. **Feature Irrelevance**: In high dimensions, many features may not be relevant to the output variable. KNN does not inherently perform feature selection, so it considers all features equally, which can dilute the impact of important features and allow irrelevant features to affect the outcome.\n",
    "\n",
    "    4. **Increased Computational Complexity**: As the number of dimensions increases, the computational burden of calculating distances between points also increases. This higher computational complexity can make KNN impractical for very high-dimensional data.\n",
    "\n",
    "    5. **Risk of Overfitting**: In high-dimensional spaces, KNN can suffer from overfitting, where the model captures noise in the training data rather than the underlying distribution. This overfitting can occur because in high dimensions, a given point might be \"near\" its nearest neighbor in terms of Euclidean distance but still not be \"close\" in a meaningful way for prediction purposes.\n",
    "\n",
    "    To mitigate the effects of the curse of dimensionality in KNN, techniques such as dimensionality reduction (e.g., PCA, t-SNE), feature selection, or increasing the sample size can be employed. However, these techniques have their limitations and may not always be feasible, depending on the specific data and context.\n",
    "\n",
    "5. **Noise and Outlier Sensitivity**: KNN assumes that the dataset has little noise, few outliers, and is not heavily imbalanced, as these factors can significantly impact the performance of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67cc57-3cb4-4306-8625-98505d983992",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af727467-6a9e-4449-8b62-9856c7d8cb5c",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n",
    "\n",
    "1. **Collect Data**: Have a dataset consisting of data points with known labels (for classification) or values (for regression).\n",
    "\n",
    "2. **Feature Selection**: Choose the features that are considered relevant for predicting the output.\n",
    "\n",
    "3. **Normalization (Optional but Recommended)**: Normalize or standardize the data if the features have different scales. This is important because KNN uses distance calculations.\n",
    "\n",
    "### Step 2: Choose the Number of Neighbors ($ k $)\n",
    "\n",
    "1. Select $ k $, the number of nearest neighbors to consider in the prediction. The choice of $ k $ can significantly influence the performance of the algorithm.\n",
    "\n",
    "### Step 3: Distance Calculation\n",
    "\n",
    "For a new data point $ x $ whose label or value needs to be predicted:\n",
    "\n",
    "1. **Calculate Distance**: Compute the distance between $ x $ and each point in the training data. The most common distance metric is Euclidean distance, though others like Manhattan or Hamming can also be used. The Euclidean distance between two points $ x = (x_1, x_2, ..., x_n) $ and $ y = (y_1, y_2, ..., y_n) $ in $ n $-dimensional space is:\n",
    "   $$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "### Step 4: Find Nearest Neighbors\n",
    "\n",
    "1. **Sort by Distance**: Arrange the points in the training set in ascending order based on their distance to $ x $.\n",
    "\n",
    "2. **Select Top $ k $**: Identify the top $ k $ nearest points (neighbors) from this sorted list.\n",
    "\n",
    "### Step 5: Prediction\n",
    "\n",
    "- **For Classification**: The predicted class for $ x $ is the most common class among its $ k $ nearest neighbors. If $ k = 1 $, the class of the nearest neighbor is assigned to $ x $.\n",
    "  \n",
    "  Mathematically, it can be expressed as:\n",
    "  $$ C(x) = \\text{mode}\\{C(x_1), C(x_2), ..., C(x_k)\\} $$\n",
    "  where $ C(x_i) $ is the class of the $ i $-th nearest neighbor.\n",
    "\n",
    "- **For Regression**: The predicted value for $ x $ is typically the average (or sometimes the median) of the values of its $ k $ nearest neighbors.\n",
    "  \n",
    "  The prediction can be formulated as:\n",
    "  $$ Y(x) = \\frac{1}{k} \\sum_{i=1}^{k} Y(x_i) $$\n",
    "  where $ Y(x_i) $ is the value of the $ i $-th nearest neighbor.\n",
    "\n",
    "### Step 6: Model Evaluation (Optional)\n",
    "\n",
    "1. **Evaluate the Model**: If you have a test dataset, evaluate the accuracy of the model (for classification) or the error metric (like MSE for regression) on this test dataset.\n",
    "\n",
    "### Considerations in KNN\n",
    "\n",
    "- **Choice of $ k $**: A small $ k $ makes the algorithm sensitive to noise, while a large $ k $ makes it computationally expensive and possibly less precise.\n",
    "- **Distance Metrics**: The choice of distance metric can affect the performance.\n",
    "- **Curse of Dimensionality**: KNN can perform poorly with high-dimensional data.\n",
    "- **Weighted Neighbors**: Sometimes, assigning weights to the neighbors based on their distance can improve performance.\n",
    "\n",
    "KNN's simplicity is one of its biggest advantages, but this simplicity also means it may not perform well on datasets with high-dimensional feature spaces or where the decision boundary is very irregular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16475e03-6bf2-4f7b-8a11-2e00833ce974",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371df06b-7bf0-4a89-bc2b-5770113a9e96",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a straightforward and versatile algorithm used in both classification and regression tasks. Here are its pros and cons:\n",
    "\n",
    "### Pros of KNN:\n",
    "\n",
    "1. **Simplicity and Ease of Implementation**: KNN is easy to understand and implement, which makes it a good starting point for tackling classification and regression problems.\n",
    "\n",
    "2. **No Training Phase**: KNN is a lazy learning algorithm that doesn't require a training phase, as it doesn't build a model. Instead, it stores the dataset and performs action on the dataset at the time of making real-time predictions.\n",
    "\n",
    "3. **Flexibility in Feature Selection**: KNN can be used with a variety of data types and is quite robust to noisy data.\n",
    "\n",
    "4. **Versatility for Different Problem Types**: It works well for both classification and regression tasks.\n",
    "\n",
    "5. **Naturally Handles Multi-Class Cases**: KNN can handle scenarios where a data point may belong to more than one class.\n",
    "\n",
    "6. **Adaptability**: It can adapt to the data structure, making it useful in real-world scenarios where the decision boundary is not always clear.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Cons of KNN:\n",
    "\n",
    "1. **Computationally Intensive**: KNN requires storing the entire dataset and calculating distances for each query, which can be computationally expensive, especially with large datasets.\n",
    "\n",
    "2. **High Memory Requirement**: Since the entire dataset needs to be stored, it requires a lot of memory, making it not suitable for large datasets.\n",
    "\n",
    "3. **Sensitive to Irrelevant Features**: KNN is sensitive to the scale of the data and irrelevant features, which can significantly degrade the performance of the algorithm.\n",
    "\n",
    "4. **Curse of Dimensionality**: Its performance degrades as the number of features or dimensions grows (high-dimensional space).\n",
    "\n",
    "5. **Choosing the Right $ k $ Value**: Selecting the optimal number of neighbors, $ k $, can be challenging and significantly affects the algorithm's performance.\n",
    "\n",
    "6. **Sensitive to Imbalanced Data**: KNN can be biased towards the more frequent classes in case of imbalanced datasets.\n",
    "\n",
    "7. **Slow Query Time**: As each query involves calculating distances with every point in the dataset, the query time can be quite slow.\n",
    "\n",
    "8. **Not Well-Suited for Large Datasets**: Due to its computational and memory requirements, KNN is not well-suited for large datasets.\n",
    "\n",
    "9. **Lack of a Probabilistic Explanation**: KNN does not provide a probabilistic explanation for classifications.\n",
    "\n",
    "In summary, while KNN is a useful tool and often performs well in scenarios where the data has a clear, distinguishable structure, it may not be the best choice for large datasets or datasets with many dimensions. Additionally, careful preprocessing of data and thoughtful selection of the number of neighbors are crucial for its effective application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b6415-e86a-4869-9af4-303e87db7028",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272c544-8b4f-40e9-8c1b-386d10ba7ff3",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438e13e-b473-400f-9a38-e9a6fce5cc67",
   "metadata": {},
   "source": [
    "Approximate Nearest Neighbor (ANN) is a computational technique used in the field of computer science and data science for efficiently finding points in a dataset that are close to a given query point. This technique is particularly useful when dealing with very large datasets or high-dimensional data where exact nearest neighbor search becomes computationally expensive.\n",
    "\n",
    "### Concept of Approximate Nearest Neighbor:\n",
    "\n",
    "1. **Purpose**: The goal of ANN is to find points in a dataset that are approximately close to a query point, rather than finding the exact nearest neighbors, which can be much faster at the cost of some accuracy.\n",
    "\n",
    "2. **Principle**: ANN algorithms typically use various forms of space partitioning (like trees or hashing) to divide the dataset into smaller, manageable subregions. When a query is made, only these subregions are searched, thereby reducing the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79beb1f8-8aec-45df-ad25-ace077cb7c6e",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711f17a-f604-492a-a35f-a8fab246081e",
   "metadata": {},
   "source": [
    "1. **Distance Metrics**: Like exact nearest neighbor algorithms, ANN algorithms also assume an appropriate distance metric (such as Euclidean, Manhattan, or cosine similarity) to measure closeness between points. The choice of metric can impact the performance of the algorithm.\n",
    "\n",
    "2. **Dimensionality**: While designed to perform better in high-dimensional spaces compared to exact nearest neighbor search, ANN algorithms still assume that the dimensionality is not excessively high, as very high dimensions can lead to the curse of dimensionality, where distances become less meaningful.\n",
    "\n",
    "3. **Error Tolerance**: ANN methods operate under the assumption that a certain degree of error in finding the absolute nearest neighbor is acceptable. This tolerance is often a parameter in ANN algorithms and can be adjusted based on the application's requirements.\n",
    "\n",
    "4. **Data Distribution**: Some ANN algorithms may make assumptions about the distribution of the data, although this is not always the case. The performance of the algorithm may vary depending on how the data is distributed in the feature space.\n",
    "\n",
    "5. **Density and Sparsity**: The effectiveness of the algorithm can be influenced by the density or sparsity of the data. In very sparse datasets, finding even approximate nearest neighbors can be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d472c-e9fc-4104-8f3f-7ff3b2933d35",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8961851-846f-4856-a2e9-35790d13b780",
   "metadata": {},
   "source": [
    "The Annoy (Approximate Nearest Neighbors Oh Yeah) library implements an efficient algorithm for Approximate Nearest Neighbor (ANN) searches, particularly suitable for high-dimensional spaces. The core of the algorithm is based on building a forest of Random Projection Trees. Here's a more detailed breakdown of the algorithm, incorporating mathematical concepts where necessary:\n",
    "\n",
    "### Algorithm of Annoy Library for ANN\n",
    "\n",
    "1. **Random Projection Trees Construction**:\n",
    "   - The dataset with vectors $ \\{x_1, x_2, ..., x_n\\} $ in a high-dimensional space is given.\n",
    "   - For each tree in the forest:\n",
    "     - Select a pivot: Choose two random data points, $ x_i $ and $ x_j $, from the dataset.\n",
    "     - Create a hyperplane that splits the space into two by bisecting the line segment between $ x_i $ and $ x_j $.\n",
    "     - The hyperplane can be represented by the equation $ (x_i - x_j) \\cdot x + b = 0 $, where $ b $ is the bias term.\n",
    "     - Partition the dataset based on which side of the hyperplane each point lies.\n",
    "     - Recursively apply this process to each partition, creating a binary tree structure until a specified tree depth is reached or the number of points in a node falls below a threshold.\n",
    "\n",
    "2. **Index Building**:\n",
    "   - For each item in the dataset, insert it into each tree.\n",
    "   - Traverse down the tree according to which side of the hyperplane the item lies on at each node, until reaching a leaf node.\n",
    "   - Store the item in the leaf node.\n",
    "\n",
    "3. **Searching for Nearest Neighbors**:\n",
    "   - Given a query point $ q $, perform the search to find its approximate nearest neighbors.\n",
    "   - For each tree in the forest:\n",
    "     - Traverse the tree with the query point $ q $, as done during the index building, to reach a leaf node.\n",
    "     - Retrieve all items stored in this leaf node as candidate neighbors.\n",
    "   - Use a priority queue (or similar data structure) to maintain the closest candidates based on their distance to the query point $ q $.\n",
    "   - The distance metric (often Euclidean) is calculated as $ d(x, q) = \\sqrt{\\sum_{i=1}^{D} (x_i - q_i)^2} $, where $ D $ is the dimensionality of the space.\n",
    "   - Optionally, inspect more nodes by backtracking the traversal (this depends on the search parameters like `search_k` in Annoy).\n",
    "\n",
    "4. **Result Compilation**:\n",
    "   - After traversing all trees, compile the list of candidate neighbors.\n",
    "   - Sort these candidates based on their distance to the query point.\n",
    "   - Return the top $ k $ nearest neighbors as the final result.\n",
    "\n",
    "### Considerations in Annoy's Algorithm:\n",
    "\n",
    "- **Randomness**: The use of random hyperplanes in tree construction ensures diversity in the forest, which helps in efficiently covering the search space.\n",
    "- **Parameters**: The number of trees and the maximum number of nodes to inspect (`search_k`) are crucial parameters that balance between accuracy and performance.\n",
    "- **Approximation**: The method provides approximate results, trading off accuracy for speed and efficiency, which is especially beneficial in high-dimensional spaces.\n",
    "\n",
    "Annoy's algorithm effectively addresses the challenge of finding nearest neighbors in high-dimensional spaces by using an ensemble of random projection trees, making the search process both fast and memory-efficient. This approach makes it a popular choice in various applications, including recommendation systems, image search, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2bc460-5902-4eff-a9a5-87b90d32f62f",
   "metadata": {},
   "source": [
    "## Comparison between KNN and ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530b524-3fcf-4019-9fbf-dbdbea08bce1",
   "metadata": {},
   "source": [
    "Comparing K-Nearest Neighbors (KNN) and Approximate Nearest Neighbor (ANN) involves looking at their operational principles, use cases, strengths, and limitations. Both are algorithms used for finding nearest neighbors in a dataset, but they have different approaches and are suitable for different scenarios.\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "1. **Operational Principle**: KNN is a non-parametric, instance-based learning algorithm. It finds the exact nearest neighbors to a query point based on a distance metric (like Euclidean distance).\n",
    "\n",
    "2. **Accuracy**: Provides exact results by considering all data points.\n",
    "\n",
    "3. **Performance**: Computationally expensive, especially with large datasets or high-dimensional data, as it requires calculating distances to all data points.\n",
    "\n",
    "4. **Memory Requirement**: Needs to store the entire dataset.\n",
    "\n",
    "5. **Scalability**: Does not scale well with large datasets.\n",
    "\n",
    "6. **Use Cases**: Suitable for small to medium-sized datasets and in cases where accuracy is paramount over speed.\n",
    "\n",
    "7. **Flexibility**: Works with various types of data (numerical, categorical) but requires appropriate distance metrics for each type.\n",
    "\n",
    "8. **Implementation**: Straightforward to implement and understand.\n",
    "\n",
    "### Approximate Nearest Neighbor (ANN)\n",
    "\n",
    "1. **Operational Principle**: ANN, as implemented in libraries like Annoy, FLANN, or FAISS, uses various optimization algorithms (like random projection trees) to find approximate nearest neighbors efficiently.\n",
    "\n",
    "2. **Accuracy**: Trades off some accuracy for speed. It finds neighbors that are close to the query point but not necessarily the exact nearest neighbors.\n",
    "\n",
    "3. **Performance**: Much faster on large datasets or high-dimensional data compared to KNN, especially for approximate results.\n",
    "\n",
    "4. **Memory Requirement**: Typically requires less memory than KNN, as not all data points need to be stored in memory for searching.\n",
    "\n",
    "5. **Scalability**: Scales well with large datasets and high-dimensional data.\n",
    "\n",
    "6. **Use Cases**: Ideal for applications where speed is critical and exact precision is not necessary, such as recommendation systems and real-time data processing.\n",
    "\n",
    "7. **Flexibility**: Primarily designed for numerical data in high-dimensional spaces.\n",
    "\n",
    "8. **Implementation**: More complex to implement and understand than KNN. Requires tuning of parameters like the number of trees or the search space.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "- **Accuracy vs. Speed**: KNN gives accurate results but is slower, especially as data size grows. ANN provides faster queries but at the cost of some accuracy.\n",
    "- **Scalability**: ANN is more scalable to large datasets and high-dimensional data.\n",
    "- **Use Cases**: KNN is preferred for smaller datasets where accuracy is critical, while ANN is suitable for larger datasets where speed is more important.\n",
    "\n",
    "In summary, the choice between KNN and ANN depends on the specific requirements of your task, particularly the trade-off between accuracy and speed, as well as the size and dimensionality of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acadf2cc-24ad-4fe7-bba0-19b7fd3e058c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a8853-c9d2-4143-929d-5e119fba2df4",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c511c1-c016-42b1-b09c-606528de1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import load_dataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f670e38-c91d-42b3-9a59-4274c9ae6ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>...</th>\n",
       "      <th>glyburide-metformin:Up</th>\n",
       "      <th>A1Cresult:&gt;7</th>\n",
       "      <th>A1Cresult:&gt;8</th>\n",
       "      <th>A1Cresult:None</th>\n",
       "      <th>A1Cresult:Norm</th>\n",
       "      <th>max_glu_serum:&gt;200</th>\n",
       "      <th>max_glu_serum:&gt;300</th>\n",
       "      <th>max_glu_serum:None</th>\n",
       "      <th>max_glu_serum:Norm</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
       "0               2.0                38.0             3.0             27.0   \n",
       "1               4.0                48.0             0.0             11.0   \n",
       "2               2.0                28.0             0.0             15.0   \n",
       "3               4.0                44.0             0.0             10.0   \n",
       "4               3.0                54.0             0.0              8.0   \n",
       "\n",
       "   number_outpatient  number_emergency  number_inpatient  number_diagnoses  \\\n",
       "0                0.0               1.0               2.0               7.0   \n",
       "1                0.0               0.0               0.0               9.0   \n",
       "2                0.0               3.0               4.0               9.0   \n",
       "3                0.0               0.0               0.0               7.0   \n",
       "4                0.0               0.0               0.0               8.0   \n",
       "\n",
       "   change  diabetesMed  ...  glyburide-metformin:Up  A1Cresult:>7  \\\n",
       "0     1.0          1.0  ...                     0.0           0.0   \n",
       "1     0.0          0.0  ...                     0.0           0.0   \n",
       "2     0.0          1.0  ...                     0.0           0.0   \n",
       "3     0.0          1.0  ...                     0.0           0.0   \n",
       "4     1.0          1.0  ...                     0.0           0.0   \n",
       "\n",
       "   A1Cresult:>8  A1Cresult:None  A1Cresult:Norm  max_glu_serum:>200  \\\n",
       "0           0.0             1.0             0.0                 0.0   \n",
       "1           0.0             0.0             1.0                 0.0   \n",
       "2           0.0             1.0             0.0                 0.0   \n",
       "3           0.0             1.0             0.0                 0.0   \n",
       "4           0.0             1.0             0.0                 0.0   \n",
       "\n",
       "   max_glu_serum:>300  max_glu_serum:None  max_glu_serum:Norm  readmitted  \n",
       "0                 0.0                 1.0                 0.0           0  \n",
       "1                 0.0                 1.0                 0.0           0  \n",
       "2                 0.0                 1.0                 0.0           1  \n",
       "3                 0.0                 1.0                 0.0           0  \n",
       "4                 0.0                 1.0                 0.0           0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imodels/diabetes-readmission\", split='train')\n",
    "df = dataset.to_pandas()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7d6439-83fb-4771-a93c-00cd073bcdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df.iloc[0:1000,0:5])\n",
    "y = np.array(df.iloc[0:1000,-1])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7dadf6-f49c-4a2d-8b42-03497216236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc04d58f-35c2-4c50-85c3-2b6e0d927384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[63 39]\n",
      " [51 47]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58       102\n",
      "           1       0.55      0.48      0.51        98\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "y_pred = knn(X_train, y_train, X_test, 10)\n",
    "\n",
    "# Evaluating the classifier\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60f839-d123-46db-a25f-77954342d45b",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c660b68c-991a-4623-a761-2e962266d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[76 26]\n",
      " [60 38]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64       102\n",
      "           1       0.59      0.39      0.47        98\n",
      "\n",
      "    accuracy                           0.57       200\n",
      "   macro avg       0.58      0.57      0.55       200\n",
      "weighted avg       0.58      0.57      0.56       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Training the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff517c24-79ac-43db-b82a-07725b1d66cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
