{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6960bb9d-bdd0-4b19-b413-555f940ab214",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111a0b2-be1e-4130-873b-bf81bc5dd688",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcda825-78f2-4a4e-a195-d026fd1f709b",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like tree structure where an internal node represents a feature (or attribute), a branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner called recursive partitioning.\n",
    "\n",
    "### Structure\n",
    "\n",
    "- **Root Node**: Represents the entire dataset, which gets divided into two or more homogeneous sets.\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes.\n",
    "- **Leaf Node**: Nodes with no children (no further split).\n",
    "- **Branch**: Represents a decision rule, leading to a decision or leaf node.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy measures the randomness or uncertainty in the information. It is defined as:\n",
    "\n",
    "$$ H(S) = -\\sum_{x} P(x) \\log_2 P(x) $$\n",
    "\n",
    "Where:\n",
    "- $ H(S) $ is the entropy of set $ S $.\n",
    "- $ P(x) $ is the proportion of the number of elements in class $ x $ to the number of elements in set $ S $.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information Gain measures the reduction in entropy or surprise by splitting a dataset on an attribute. It is calculated as:\n",
    "\n",
    "$$ IG(S, A) = H(S) - \\sum \\left( \\frac{|S_v|}{|S|} \\right) H(S_v) $$\n",
    "\n",
    "Where:\n",
    "- $ IG(S, A) $ is the information gain by splitting set $ S $ on attribute $ A $.\n",
    "- $ H(S) $ is the entropy of set $ S $.\n",
    "- $ S_v $ is the subset of $ S $ for each value $ v $ of attribute $ A $.\n",
    "- $ |S_v|/|S| $ is the proportion of the number of elements in $ S_v $ to the number of elements in set $ S $.\\\n",
    "\n",
    "### Gini Index\n",
    "$$ \\text{Gini}(D) = 1 - \\sum_{i=1}^{m} P_i^2 $$\n",
    "$$ \\text{Gini}(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} \\text{Gini}(D_j) $$\n",
    "\n",
    "Where:\n",
    "- $ P_i $ is the proportion of the number of elements in class $ i $ to the number of elements in set $ D $.\n",
    "- $ D_j $ represents the subset of $ D $ for each value $ j $ of feature $ F $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0ed9b-b946-4700-92a9-1c6ac85cb519",
   "metadata": {},
   "source": [
    "## Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c94ca5-7b3e-4e1f-9bd9-4e001bf26102",
   "metadata": {},
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada623f-78ee-4f9c-b0b1-d8c54deb46e0",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674475d-8076-4c84-86b1-f035d61d5b18",
   "metadata": {},
   "source": [
    "1. **Initialization**: Start with the entire dataset, $ D $, at the root node.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - For each feature $ F $ in the dataset, calculate the split criterion (such as Entropy or Gini index) for each possible split value.\n",
    "   - The split criterion for a binary classification problem can be calculated as follows:\n",
    "\n",
    "     **Entropy**:\n",
    "     $$ H(D) = -\\sum_{i=1}^{m} P_i \\log_2 P_i $$\n",
    "     $$ H(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} H(D_j) $$\n",
    "     $$ \\text{Information Gain} = H(D) - H(D, F) $$\n",
    "\n",
    "     **Gini Index**:\n",
    "     $$ \\text{Gini}(D) = 1 - \\sum_{i=1}^{m} P_i^2 $$\n",
    "     $$ \\text{Gini}(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} \\text{Gini}(D_j) $$\n",
    "\n",
    "     Where:\n",
    "     - $ P_i $ is the proportion of the number of elements in class $ i $ to the number of elements in set $ D $.\n",
    "     - $ D_j $ represents the subset of $ D $ for each value $ j $ of feature $ F $.\n",
    "\n",
    "3. **Best Feature Selection**:\n",
    "   - Choose the feature with the highest information gain (or the lowest Gini index) as the splitting feature.\n",
    "   - Let $ F_{\\text{best}} $ be this feature.\n",
    "\n",
    "4. **Node Creation**:\n",
    "   - Create a decision node that splits on $ F_{\\text{best}} $.\n",
    "\n",
    "5. **Dataset Splitting**:\n",
    "   - Split the dataset $ D $ into subsets $ D_1, D_2, \\ldots, D_k $ based on the unique values of $ F_{\\text{best}} $.\n",
    "\n",
    "6. **Recursion**:\n",
    "   - For each subset $ D_i $:\n",
    "     - If $ D_i $ is pure (contains only one class), attach a leaf node to the decision node with that class label.\n",
    "     - If $ D_i $ still contains mixed classes, repeat steps 2 to 6 for $ D_i $.\n",
    "\n",
    "7. **Termination**:\n",
    "   - The process continues until all data is classified or another stopping criterion is met (such as maximum tree depth).\n",
    "\n",
    "### Example\n",
    "\n",
    "Here's the dataset:\n",
    "\n",
    "| Weather | Wind   | Play |\n",
    "|---------|--------|------|\n",
    "| Sunny   | Weak   | Yes  |\n",
    "| Sunny   | Strong | No   |\n",
    "| Rainy   | Weak   | Yes  |\n",
    "| Rainy   | Strong | No   |\n",
    "| Sunny   | Weak   | Yes  |\n",
    "\n",
    "First, we calculate the entropy and Gini index for the entire dataset (root node), and then for one of the features.\n",
    "\n",
    "#### Entropy Calculation\n",
    "\n",
    "1. **Entropy for the entire dataset:**\n",
    "\n",
    "   There are 3 'Yes' and 2 'No' outcomes.\n",
    "\n",
    "   $$ P(Yes) = \\frac{3}{5}, \\quad P(No) = \\frac{2}{5} $$\n",
    "\n",
    "   $$ H(S) = -P(Yes) \\log_2 P(Yes) - P(No) \\log_2 P(No) $$\n",
    "   $$ H(S) = -\\frac{3}{5} \\log_2 \\frac{3}{5} - \\frac{2}{5} \\log_2 \\frac{2}{5} $$\n",
    "\n",
    "2. **Entropy for a feature (e.g., Weather):**\n",
    "\n",
    "   For 'Sunny' (3 instances): 2 'Yes', 1 'No'\n",
    "   $$ H(Sunny) = -\\frac{2}{3} \\log_2 \\frac{2}{3} - \\frac{1}{3} \\log_2 \\frac{1}{3} $$\n",
    "\n",
    "   For 'Rainy' (2 instances): 1 'Yes', 1 'No'\n",
    "   $$ H(Rainy) = -\\frac{1}{2} \\log_2 \\frac{1}{2} - \\frac{1}{2} \\log_2 \\frac{1}{2} $$\n",
    "\n",
    "   Weighted average:\n",
    "   $$ H(Weather) = \\frac{3}{5} H(Sunny) + \\frac{2}{5} H(Rainy) $$\n",
    "\n",
    "#### Gini Index Calculation\n",
    "\n",
    "1. **Gini index for the entire dataset:**\n",
    "   $$ Gini(S) = 1 - [P(Yes)^2 + P(No)^2] $$\n",
    "   $$ Gini(S) = 1 - \\left[ \\left(\\frac{3}{5}\\right)^2 + \\left(\\frac{2}{5}\\right)^2 \\right] $$\n",
    "\n",
    "2. **Gini index for a feature (e.g., Weather):**\n",
    "\n",
    "   For 'Sunny':\n",
    "   $$ Gini(Sunny) = 1 - \\left[ \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 \\right] $$\n",
    "\n",
    "   For 'Rainy':\n",
    "   $$ Gini(Rainy) = 1 - \\left[ \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right] $$\n",
    "\n",
    "   Weighted average:\n",
    "   $$ Gini(Weather) = \\frac{3}{5} Gini(Sunny) + \\frac{2}{5} Gini(Rainy) $$\n",
    "\n",
    "Let's calculate these values.\n",
    "\n",
    "Here are the calculated values for entropy and the Gini index:\n",
    "\n",
    "1. **Entropy for the entire dataset (root node):** $ H(S) = 0.971 $\n",
    "2. **Weighted average entropy for the 'Weather' feature:** $ H(Weather) = 0.951 $\n",
    "\n",
    "3. **Gini index for the entire dataset (root node):** $ Gini(S) = 0.48 $\n",
    "4. **Weighted average Gini index for the 'Weather' feature:** $ Gini(Weather) = 0.467 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fbe01-600f-47a6-9526-802a8338490d",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96be45-f87d-4c21-9667-2987ce5b5967",
   "metadata": {},
   "source": [
    "### Pros of Decision Trees:\n",
    "\n",
    "1. **Easy to Understand and Interpret**: Trees can be visualised, which makes them easy to interpret and explain, even to non-technical stakeholders.\n",
    "\n",
    "2. **Handles Both Numerical and Categorical Data**: They can handle datasets with a variety of variable types.\n",
    "\n",
    "3. **No Need for Data Normalization**: They don't require data to be normalized or scaled.\n",
    "\n",
    "4. **Performs Well with Large Datasets**: Can handle large datasets efficiently and are scalable.\n",
    "\n",
    "5. **Automatic Feature Selection**: The tree can select the most informative features from a dataset.\n",
    "\n",
    "6. **Non-Parametric Method**: They don't make any assumptions about the space distribution and the structure of the classifier.\n",
    "\n",
    "7. **Flexible and Versatile**: Can be used for both classification and regression tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Cons of Decision Trees:\n",
    "\n",
    "1. **Overfitting**: Without proper tuning, decision trees can easily overfit, especially with a lot of features or complex structures.\n",
    "\n",
    "2. **Not Suitable for Linear Problems**: They are not the best choice for tasks where relationships between features are linear.\n",
    "\n",
    "3. **Instability**: Small changes in the data can lead to a completely different tree.\n",
    "\n",
    "4. **Greedy Algorithms**: Decision tree algorithms are greedy, meaning they might not produce the globally optimal tree.\n",
    "\n",
    "5. **Bias Towards Dominant Classes**: Trees can be biased toward the dominant classes, leading to imbalanced classification.\n",
    "\n",
    "6. **Poor Performance on XOR, Parity, or Multiplexer Problems**: Decision trees struggle with these types of problems because they cannot easily express the XOR, parity, or multiplexer functions.\n",
    "\n",
    "7. **Complex Trees Can Be Hard to Interpret**: While simple trees are interpretable, very large and complex trees can become difficult to understand and interpret.\n",
    "\n",
    "In summary, decision trees are a powerful tool with a natural ability to explain their predictions, but they can suffer from overfitting and instability. They work best when the dataset is not too complex, and the relationships between features are not linear. Balancing the depth of the tree and the number of features considered at each split is key to building a well-performing decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91874a21-cec2-4f77-acbb-f1cfda355073",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e435d7-7af9-4628-9c26-ded9b1c60ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
