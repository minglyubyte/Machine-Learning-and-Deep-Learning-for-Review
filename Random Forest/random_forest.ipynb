{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6960bb9d-bdd0-4b19-b413-555f940ab214",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111a0b2-be1e-4130-873b-bf81bc5dd688",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcda825-78f2-4a4e-a195-d026fd1f709b",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like tree structure where an internal node represents a feature (or attribute), a branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner called recursive partitioning.\n",
    "\n",
    "### Structure\n",
    "\n",
    "- **Root Node**: Represents the entire dataset, which gets divided into two or more homogeneous sets.\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes.\n",
    "- **Leaf Node**: Nodes with no children (no further split).\n",
    "- **Branch**: Represents a decision rule, leading to a decision or leaf node.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy measures the randomness or uncertainty in the information. It is defined as:\n",
    "\n",
    "$$ H(S) = -\\sum_{x} P(x) \\log_2 P(x) $$\n",
    "\n",
    "Where:\n",
    "- $ H(S) $ is the entropy of set $ S $.\n",
    "- $ P(x) $ is the proportion of the number of elements in class $ x $ to the number of elements in set $ S $.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information Gain measures the reduction in entropy or surprise by splitting a dataset on an attribute. It is calculated as:\n",
    "\n",
    "$$ IG(S, A) = H(S) - \\sum \\left( \\frac{|S_v|}{|S|} \\right) H(S_v) $$\n",
    "\n",
    "Where:\n",
    "- $ IG(S, A) $ is the information gain by splitting set $ S $ on attribute $ A $.\n",
    "- $ H(S) $ is the entropy of set $ S $.\n",
    "- $ S_v $ is the subset of $ S $ for each value $ v $ of attribute $ A $.\n",
    "- $ |S_v|/|S| $ is the proportion of the number of elements in $ S_v $ to the number of elements in set $ S $.\\\n",
    "\n",
    "### Gini Index\n",
    "$$ \\text{Gini}(D) = 1 - \\sum_{i=1}^{m} P_i^2 $$\n",
    "$$ \\text{Gini}(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} \\text{Gini}(D_j) $$\n",
    "\n",
    "Where:\n",
    "- $ P_i $ is the proportion of the number of elements in class $ i $ to the number of elements in set $ D $.\n",
    "- $ D_j $ represents the subset of $ D $ for each value $ j $ of feature $ F $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0ed9b-b946-4700-92a9-1c6ac85cb519",
   "metadata": {},
   "source": [
    "## Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c94ca5-7b3e-4e1f-9bd9-4e001bf26102",
   "metadata": {},
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada623f-78ee-4f9c-b0b1-d8c54deb46e0",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674475d-8076-4c84-86b1-f035d61d5b18",
   "metadata": {},
   "source": [
    "1. **Initialization**: Start with the entire dataset, $ D $, at the root node.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - For each feature $ F $ in the dataset, calculate the split criterion (such as Entropy or Gini index) for each possible split value.\n",
    "   - The split criterion for a binary classification problem can be calculated as follows:\n",
    "\n",
    "     **Entropy**:\n",
    "     $$ H(D) = -\\sum_{i=1}^{m} P_i \\log_2 P_i $$\n",
    "     $$ H(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} H(D_j) $$\n",
    "     $$ \\text{Information Gain} = H(D) - H(D, F) $$\n",
    "\n",
    "     **Gini Index**:\n",
    "     $$ \\text{Gini}(D) = 1 - \\sum_{i=1}^{m} P_i^2 $$\n",
    "     $$ \\text{Gini}(D, F) = \\sum_{j \\in \\text{values}(F)} \\frac{|D_j|}{|D|} \\text{Gini}(D_j) $$\n",
    "\n",
    "     Where:\n",
    "     - $ P_i $ is the proportion of the number of elements in class $ i $ to the number of elements in set $ D $.\n",
    "     - $ D_j $ represents the subset of $ D $ for each value $ j $ of feature $ F $.\n",
    "\n",
    "3. **Best Feature Selection**:\n",
    "   - Choose the feature with the highest information gain (or the lowest Gini index) as the splitting feature.\n",
    "   - Let $ F_{\\text{best}} $ be this feature.\n",
    "\n",
    "4. **Node Creation**:\n",
    "   - Create a decision node that splits on $ F_{\\text{best}} $.\n",
    "\n",
    "5. **Dataset Splitting**:\n",
    "   - Split the dataset $ D $ into subsets $ D_1, D_2, \\ldots, D_k $ based on the unique values of $ F_{\\text{best}} $.\n",
    "\n",
    "6. **Recursion**:\n",
    "   - For each subset $ D_i $:\n",
    "     - If $ D_i $ is pure (contains only one class), attach a leaf node to the decision node with that class label.\n",
    "     - If $ D_i $ still contains mixed classes, repeat steps 2 to 6 for $ D_i $.\n",
    "\n",
    "7. **Termination**:\n",
    "   - The process continues until all data is classified or another stopping criterion is met (such as maximum tree depth).\n",
    "\n",
    "### Example\n",
    "\n",
    "Here's the dataset:\n",
    "\n",
    "| Weather | Wind   | Play |\n",
    "|---------|--------|------|\n",
    "| Sunny   | Weak   | Yes  |\n",
    "| Sunny   | Strong | No   |\n",
    "| Rainy   | Weak   | Yes  |\n",
    "| Rainy   | Strong | No   |\n",
    "| Sunny   | Weak   | Yes  |\n",
    "\n",
    "First, we calculate the entropy and Gini index for the entire dataset (root node), and then for one of the features.\n",
    "\n",
    "#### Entropy Calculation\n",
    "\n",
    "1. **Entropy for the entire dataset:**\n",
    "\n",
    "   There are 3 'Yes' and 2 'No' outcomes.\n",
    "\n",
    "   $$ P(Yes) = \\frac{3}{5}, \\quad P(No) = \\frac{2}{5} $$\n",
    "\n",
    "   $$ H(S) = -P(Yes) \\log_2 P(Yes) - P(No) \\log_2 P(No) $$\n",
    "   $$ H(S) = -\\frac{3}{5} \\log_2 \\frac{3}{5} - \\frac{2}{5} \\log_2 \\frac{2}{5} $$\n",
    "\n",
    "2. **Entropy for a feature (e.g., Weather):**\n",
    "\n",
    "   For 'Sunny' (3 instances): 2 'Yes', 1 'No'\n",
    "   $$ H(Sunny) = -\\frac{2}{3} \\log_2 \\frac{2}{3} - \\frac{1}{3} \\log_2 \\frac{1}{3} $$\n",
    "\n",
    "   For 'Rainy' (2 instances): 1 'Yes', 1 'No'\n",
    "   $$ H(Rainy) = -\\frac{1}{2} \\log_2 \\frac{1}{2} - \\frac{1}{2} \\log_2 \\frac{1}{2} $$\n",
    "\n",
    "   Weighted average:\n",
    "   $$ H(Weather) = \\frac{3}{5} H(Sunny) + \\frac{2}{5} H(Rainy) $$\n",
    "\n",
    "#### Gini Index Calculation\n",
    "\n",
    "1. **Gini index for the entire dataset:**\n",
    "   $$ Gini(S) = 1 - [P(Yes)^2 + P(No)^2] $$\n",
    "   $$ Gini(S) = 1 - \\left[ \\left(\\frac{3}{5}\\right)^2 + \\left(\\frac{2}{5}\\right)^2 \\right] $$\n",
    "\n",
    "2. **Gini index for a feature (e.g., Weather):**\n",
    "\n",
    "   For 'Sunny':\n",
    "   $$ Gini(Sunny) = 1 - \\left[ \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 \\right] $$\n",
    "\n",
    "   For 'Rainy':\n",
    "   $$ Gini(Rainy) = 1 - \\left[ \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right] $$\n",
    "\n",
    "   Weighted average:\n",
    "   $$ Gini(Weather) = \\frac{3}{5} Gini(Sunny) + \\frac{2}{5} Gini(Rainy) $$\n",
    "\n",
    "Let's calculate these values.\n",
    "\n",
    "Here are the calculated values for entropy and the Gini index:\n",
    "\n",
    "1. **Entropy for the entire dataset (root node):** $ H(S) = 0.971 $\n",
    "2. **Weighted average entropy for the 'Weather' feature:** $ H(Weather) = 0.951 $\n",
    "\n",
    "3. **Gini index for the entire dataset (root node):** $ Gini(S) = 0.48 $\n",
    "4. **Weighted average Gini index for the 'Weather' feature:** $ Gini(Weather) = 0.467 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fbe01-600f-47a6-9526-802a8338490d",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96be45-f87d-4c21-9667-2987ce5b5967",
   "metadata": {},
   "source": [
    "### Pros of Decision Trees:\n",
    "\n",
    "1. **Easy to Understand and Interpret**: Trees can be visualised, which makes them easy to interpret and explain, even to non-technical stakeholders.\n",
    "\n",
    "2. **Handles Both Numerical and Categorical Data**: They can handle datasets with a variety of variable types.\n",
    "\n",
    "3. **No Need for Data Normalization**: They don't require data to be normalized or scaled.\n",
    "\n",
    "4. **Performs Well with Large Datasets**: Can handle large datasets efficiently and are scalable.\n",
    "\n",
    "5. **Automatic Feature Selection**: The tree can select the most informative features from a dataset.\n",
    "\n",
    "6. **Non-Parametric Method**: They don't make any assumptions about the space distribution and the structure of the classifier.\n",
    "\n",
    "7. **Flexible and Versatile**: Can be used for both classification and regression tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Cons of Decision Trees:\n",
    "\n",
    "1. **Overfitting**: Without proper tuning, decision trees can easily overfit, especially with a lot of features or complex structures.\n",
    "\n",
    "2. **Not Suitable for Linear Problems**: They are not the best choice for tasks where relationships between features are linear.\n",
    "\n",
    "3. **Instability**: Small changes in the data can lead to a completely different tree.\n",
    "\n",
    "4. **Greedy Algorithms**: Decision tree algorithms are greedy, meaning they might not produce the globally optimal tree.\n",
    "\n",
    "5. **Bias Towards Dominant Classes**: Trees can be biased toward the dominant classes, leading to imbalanced classification.\n",
    "\n",
    "6. **Poor Performance on XOR, Parity, or Multiplexer Problems**: Decision trees struggle with these types of problems because they cannot easily express the XOR, parity, or multiplexer functions.\n",
    "\n",
    "7. **Complex Trees Can Be Hard to Interpret**: While simple trees are interpretable, very large and complex trees can become difficult to understand and interpret.\n",
    "\n",
    "In summary, decision trees are a powerful tool with a natural ability to explain their predictions, but they can suffer from overfitting and instability. They work best when the dataset is not too complex, and the relationships between features are not linear. Balancing the depth of the tree and the number of features considered at each split is key to building a well-performing decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91874a21-cec2-4f77-acbb-f1cfda355073",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed7867-b80a-49ed-a907-1eb1dd8d2987",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "Random Forest is an ensemble learning method used for both classification and regression tasks. It builds upon the concept of decision trees, combining multiple trees to improve the overall performance and accuracy.\n",
    "\n",
    "- **Ensemble of Decision Trees**: Random Forest creates a 'forest' of decision trees, each trained on a random subset of the data.\n",
    "- **Bagging (Bootstrap Aggregating)**: Each tree is grown on a bootstrap sample, which is a random sample of the training data drawn with replacement.\n",
    "- **Feature Randomness**: When splitting a node during the construction of the tree, the best split is found from a random subset of the features, not from all features.\n",
    "- **Majority Voting (Classification)** or **Average Prediction (Regression)**: For classification, the prediction of the Random Forest is the mode of the classes predicted by individual trees. For regression, it is the average of the predictions.\n",
    "\n",
    "Given a training set $ X = x_1, x_2, ..., x_n $ with responses $ Y = y_1, y_2, ..., y_n $, the Random Forest algorithm involves the following steps:\n",
    "\n",
    "1. For $ B $ bootstrap samples (where $ B $ is the number of trees in the forest):\n",
    "   - Draw a bootstrap sample $ X_b, Y_b $ from the training data.\n",
    "   - Grow a decision tree $ f_b $ on $ X_b, Y_b $, splitting nodes by evaluating only a randomly selected subset of the features at each step. (select a random subset of features for each node)\n",
    "\n",
    "2. Output the ensemble of trees $ \\{ f_1, f_2, ..., f_B \\} $.\n",
    "\n",
    "3. To make a prediction for a new point $ x $:\n",
    "   - **Classification**: Let each tree $ f_b $ vote and take the majority vote as the final prediction.\n",
    "   - **Regression**: Average the predictions of each tree.\n",
    "\n",
    "   $$ \\hat{f}(x) = \\frac{1}{B} \\sum_{b=1}^{B} f_b(x) $$ (for regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26977b-c48b-4c3d-92a3-895b8b3db35d",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "### Pros\n",
    "\n",
    "- **High Accuracy**: Combining multiple trees reduces the variance, often leading to better predictions.\n",
    "- **Handles Overfitting**: The randomization elements help to reduce overfitting.\n",
    "- **Works Well on Large Datasets**: Effective for datasets with a large number of features and data points.\n",
    "- **Handles Missing Values**: Can handle missing values in the data.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Model Interpretability**: More complex and harder to interpret than individual decision trees.\n",
    "- **Computationally Intensive**: Requires more computational resources and time to train, especially with a large number of trees.\n",
    "- **Not Ideal for Real-time Predictions**: Due to the size of the model, it might not be the best choice for applications requiring real-time predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a089ca-e2e9-492d-8f90-a946804584b7",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330c2b68-ead4-498e-a65f-ae2db4608cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b60d00-d0a0-4273-bb26-29dadee080f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>...</th>\n",
       "      <th>glyburide-metformin:Up</th>\n",
       "      <th>A1Cresult:&gt;7</th>\n",
       "      <th>A1Cresult:&gt;8</th>\n",
       "      <th>A1Cresult:None</th>\n",
       "      <th>A1Cresult:Norm</th>\n",
       "      <th>max_glu_serum:&gt;200</th>\n",
       "      <th>max_glu_serum:&gt;300</th>\n",
       "      <th>max_glu_serum:None</th>\n",
       "      <th>max_glu_serum:Norm</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
       "0               2.0                38.0             3.0             27.0   \n",
       "1               4.0                48.0             0.0             11.0   \n",
       "2               2.0                28.0             0.0             15.0   \n",
       "3               4.0                44.0             0.0             10.0   \n",
       "4               3.0                54.0             0.0              8.0   \n",
       "\n",
       "   number_outpatient  number_emergency  number_inpatient  number_diagnoses  \\\n",
       "0                0.0               1.0               2.0               7.0   \n",
       "1                0.0               0.0               0.0               9.0   \n",
       "2                0.0               3.0               4.0               9.0   \n",
       "3                0.0               0.0               0.0               7.0   \n",
       "4                0.0               0.0               0.0               8.0   \n",
       "\n",
       "   change  diabetesMed  ...  glyburide-metformin:Up  A1Cresult:>7  \\\n",
       "0     1.0          1.0  ...                     0.0           0.0   \n",
       "1     0.0          0.0  ...                     0.0           0.0   \n",
       "2     0.0          1.0  ...                     0.0           0.0   \n",
       "3     0.0          1.0  ...                     0.0           0.0   \n",
       "4     1.0          1.0  ...                     0.0           0.0   \n",
       "\n",
       "   A1Cresult:>8  A1Cresult:None  A1Cresult:Norm  max_glu_serum:>200  \\\n",
       "0           0.0             1.0             0.0                 0.0   \n",
       "1           0.0             0.0             1.0                 0.0   \n",
       "2           0.0             1.0             0.0                 0.0   \n",
       "3           0.0             1.0             0.0                 0.0   \n",
       "4           0.0             1.0             0.0                 0.0   \n",
       "\n",
       "   max_glu_serum:>300  max_glu_serum:None  max_glu_serum:Norm  readmitted  \n",
       "0                 0.0                 1.0                 0.0           0  \n",
       "1                 0.0                 1.0                 0.0           0  \n",
       "2                 0.0                 1.0                 0.0           1  \n",
       "3                 0.0                 1.0                 0.0           0  \n",
       "4                 0.0                 1.0                 0.0           0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imodels/diabetes-readmission\", split='train')\n",
    "df = dataset.to_pandas()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ecbc00-51c7-48b8-a2d2-1d8817d4b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81410, 150)\n",
      "(81410,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df.iloc[:,:-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c0888e-47a4-407c-8b09-9cfacdf2416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.558653728043238\n",
      "Random Forest Accuracy: 0.634934283257585\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770d4c9-464c-402b-939f-a29f81b5db14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
