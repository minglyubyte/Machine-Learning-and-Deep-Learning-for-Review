{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ee0bb9-1773-4f3c-8686-ff801cc32a69",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ac77b-c83a-4a20-9a64-644051ddd91a",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e984de6-6370-40bf-8d51-71e1b423a7fe",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical model and a type of regression analysis used for **binary classification tasks**, where the goal is to predict a binary outcome (usually denoted as 0 or 1, Yes or No, True or False, etc.) based on one or more independent variables (features or predictors). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69255629-4e5d-4c36-a325-369ad13f867e",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p)}}\n",
    "$$\n",
    "\n",
    "In this formula:\n",
    "\n",
    "- $(P(Y = 1|X))$represents the probability of the binary outcome **Y** being equal to 1 given the values of the independent variables **X**.\n",
    "- $\\beta$ is the intercept or bias term.\n",
    "- $\\beta_1$, $\\beta_2$, $\\ldots$, $\\beta_p$ are the coefficients associated with the independent variables $X_1$, $X_2$, $\\ldots$, $X_p$.\n",
    "\n",
    "This formula calculates the probability that the binary outcome is 1 based on a linear combination of the independent variables, transformed using the **sigmoid (logistic) function** to ensure that the predicted probability is between 0 and 1. The values of the coefficients $\\beta$ are estimated from the training data using maximum likelihood estimation.\n",
    "\n",
    "The sigmoid activation function is the following:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "In this formula:\n",
    "\n",
    "- $\\sigma(x)$ represents the sigmoid function.\n",
    "- $x$ is the input to the sigmoid function.\n",
    "- $e$ is the base of the natural logarithm (approximately equal to 2.71828)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8994339-85f8-4b93-a629-c2cb87c959ff",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d92d69-6c90-436b-9198-5bb40689bd6b",
   "metadata": {},
   "source": [
    "1. **Linearity of the Log-Odds:**\n",
    "   - Logistic regression assumes that the log-odds of the binary outcome is a linear combination of the predictor variables. The relationship between independent variables and log-odds should be linear.\n",
    "\n",
    "2. **Independence of Observations:**\n",
    "   - Observations should be independent of each other. This means that the values of the binary outcome for one observation should not be influenced by the values of the binary outcome for other observations. \n",
    "\n",
    "3. **No or Little Multicollinearity:**\n",
    "   - Multicollinearity, where predictor variables are highly correlated, should be avoided. High multicollinearity can lead to unstable coefficient estimates.\n",
    "\n",
    "4. **Large Sample Size:**\n",
    "   - A sufficiently large sample size is important for reliable parameter estimation and hypothesis testing. A guideline is to have at least 10-20 observations per predictor variable.\n",
    "\n",
    "5. **Binary or Ordinal Dependent Variable:**\n",
    "   - Logistic regression is designed for binary or ordinal (ordered categorical) dependent variables. If your dependent variable is continuous or nominal, consider other modeling techniques.\n",
    "\n",
    "7. **No Perfect Separation:**\n",
    "   - Perfect separation, where one or more independent variables perfectly predict the outcome, should be avoided. In such cases, the maximum likelihood estimates may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c791c-dfb7-45de-b6ec-595b314c8e49",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705aa21-d612-4afe-9b15-61028b404c58",
   "metadata": {},
   "source": [
    "Deriving the optimal solution for logistic regression involves finding the parameter values that best fit the data. This is typically done using a method called Maximum Likelihood Estimation (MLE). Let's go through the derivation step by step in LaTeX.\n",
    "\n",
    "Logistic Regression Model\n",
    "$$\n",
    "   P(y=1|x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "\n",
    "Likelihood Function. The likelihood function measures how likely the observed data is, given the parameters of the model. For logistic regression, the likelihood $L$is the product of probabilities assigned to all individual observations:\n",
    "$$\n",
    "   \\ell(\\theta) = \\prod_{i=1}^{n} P(y_i|x_i)^{y_i} (1 - P(y_i|x_i))^{(1 - y_i)}\n",
    "$$\n",
    "\n",
    "Log-Likelihood. It's easier to maximize the log of the likelihood function, as it turns the product into a sum, simplifying the computation. The log-likelihood $\\ell(\\theta)$ is given by:\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} \\ln(P(y_i|x_i)^{y_i}(1 - P(y_i|x_i)^{(1 - y_i)})$$\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} y_i \\ln(P(y_i|x_i)) + (1 - y_i) \\ln(1 - P(y_i|x_i))$$\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-\\theta x_i}}) + (1 - y_i) \\ln(1 - \\frac{1}{1 + e^{-\\theta x_i}})$$\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} y_i[\\ln(\\frac{1}{1+e^{-\\theta x_i}}) - \\ln(\\frac{e^{-\\theta x_i}}{1+e^{-\\theta x_i}})] + \\ln(\\frac{e^{-\\theta x_i}}{1+e^{-\\theta x_i}})$$\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} y_i\\ln(e^{\\theta x_i}) + \\ln(\\frac{1}{1+e^{\\theta x_i}})$$\n",
    "$$ln(\\ell(\\theta)) = \\sum_{i=1}^{n} y_i\\theta x_i - \\ln(1+e^{\\theta x_i})$$\n",
    "\n",
    "Get gradient of log-likehood\n",
    "$$\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^{n} y_i x_i - \\frac{1}{1+e^{\\theta x_i}}*e^{\\theta x_i}*x_i$$\n",
    "$$\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^{n} (y_i - P(y_i|x_i))x_i$$\n",
    "\n",
    "Gradient Ascent. To find the parameters $\\theta$ that maximize the log-likelihood, we use gradient ascent (the counterpart of gradient descent used in minimization problems).\n",
    "$$\n",
    "   \\theta := \\theta + \\alpha \\nabla_\\theta \\ell(\\theta)\n",
    "$$\n",
    "\n",
    "Convergence Criteria. This iterative process continues until convergence, which can be determined by a small change in the value of the log-likelihood between iterations, or until a maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e7303-897e-4f54-a842-95ca564b5c30",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641fc71-17f9-47c5-af4c-9c54ecbf1f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
